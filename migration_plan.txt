Migration Plan: Web JSON → LLM  ➜  Docs → KB → Evidence → LLM

Rule: Never refactor everything at once. Keep the system runnable after each phase.
Practice: End every phase with a small CLI command + commit.

───────────────────────────────────────────────────────────────────────────────
Phase 0 — Stabilize imports & config
Goal
- Project is importable with zero side effects and no runtime errors on import.

Tasks
1) Config (core/config.py)
- Introduce Settings (env + paths)
- No print(), no mkdir(), no validation/raises on import
- Load .env from repo root

2) Validation (core/validate.py)
- Validate required env vars (OpenAI, Serper, Telegram)
- Run validation ONLY from CLI commands that need those creds

3) FS helpers (core/fs.py)
- ensure_dirs()
- save_json/load_json (+ optional jsonl helpers)
- All directory creation occurs in CLI/pipeline init, not at import time

Files to touch
- core/config.py
- core/validate.py
- core/fs.py
- cli.py (call configure_logging + validate_settings + ensure_dirs)

Definition of Done
- `python -m daily_art.cli --help` works even without API keys.
- Importing any module does not create files/dirs and does not raise.

Commit
- "Phase 0: settings + validation + fs helpers"

───────────────────────────────────────────────────────────────────────────────
Phase 1 — Refactor “search tool” into Documents (no KB yet)
Goal
- Stop passing raw Serper JSON around.
- Convert Serper/Wikipedia outputs into Document objects.

Tasks
1) Domain objects (domain/documents.py)
- Document
- Chunk (placeholder for next phase)
- Evidence (placeholder for next phase)
- SearchResult (placeholder for next phase)

2) Serper connector (connectors/serper.py)
- search_documents(query) -> list[Document]
- search_images(query) remains (deterministic image URLs)

3) Wikipedia connector (connectors/wikipedia.py)
- get_document(query) -> Document | None

4) Minimal CLI utility
- `fetch-docs "<query>"` saves docs JSON/JSONL to data/kb/

Files to touch
- domain/documents.py (new)
- connectors/serper.py
- connectors/wikipedia.py
- cli.py (fetch-docs command)

Definition of Done
- `python -m daily_art.cli fetch-docs "Mona Lisa" --use-wiki` writes docs JSON.
- No Serper JSON reaches the generator layer.

Stop & Commit
- "Phase 1: docs ingestion from serper/wiki"

───────────────────────────────────────────────────────────────────────────────
Phase 2 — Add chunking + embeddings + vector store (KB v1)
Goal
- Build the left side of RAG:
  Document -> Chunk -> Embed -> Index -> Search -> Evidence

Tasks (implement in this order)
1) Chunking (rag/chunking.py)
- Chunker.chunk(doc: Document) -> list[Chunk]
- Stable chunk IDs; store url/title in chunk metadata

2) Embeddings (rag/embeddings.py)
- Embedder.embed_texts(texts: list[str]) -> list[list[float]]
- Embedder.embed_query(text: str) -> list[float]

3) Vector store wrapper (rag/vectordb.py)
- VectorStore.upsert(chunks, vectors)
- VectorStore.search(query_vector, top_k) -> list[SearchResult]
- Qdrant point IDs: use UUID (uuid5 from chunk_id) and keep chunk_id in payload

4) KnowledgeBase glue (rag/kb.py)
- KnowledgeBase.upsert_documents(docs) -> chunk_count
- KnowledgeBase.search(query, top_k) -> list[Evidence]

5) CLI commands
- `kb-index --docs <docs.json>` indexes into Qdrant
- `kb-search "<query>"` prints top-k evidence snippets

Files to touch
- rag/chunking.py
- rag/embeddings.py
- rag/vectordb.py
- rag/kb.py
- cli.py (kb-index, kb-search)
- docker-compose.yml (qdrant service)

Definition of Done
- `fetch-docs` -> `kb-index` -> `kb-search` works end-to-end.
- `kb-search` prints evidence text + source url/title.
- Data directory is canonical: repo_root/data/ (no duplicate daily_art/data).

Stop & Commit
- "Phase 2: KB v1 with qdrant indexing + retrieval"

───────────────────────────────────────────────────────────────────────────────
Phase 3 — Generator consumes Evidence (not Serper JSON)
Goal
- Modern RAG: the model sees evidence snippets + metadata only.

Tasks
1) Generator API change
- PostGenerator.generate(meta, evidence: list[Evidence]) -> dict
- Prompt: “Use ONLY evidence. If not supported, return empty string.”
- Output: strict JSON schema only (no URLs, no citations, no markdown)

2) Deterministic citations
- citations = citations_from_evidence(evidence) (url/title)
- Retire pick_sources(top_meta, wiki_summary)

3) Pipeline draft flow
- query -> fetch docs -> kb.upsert_documents(docs)
- evidence = kb.search(query)
- text_data = generator.generate(meta, evidence)
- assemble ArtPost with painting_urls + citations + narrative fields

Files to touch
- llm_generators.py OR connectors/llm.py
- pipeline/art_pipeline.py (draft flow)
- domain/citations.py (helper)
- deprecate: parse_serper_assets, pick_sources (keep temporarily if needed)

Definition of Done
- `draft()` works end-to-end using evidence retrieval.
- All citations originate from Evidence; no hallucinated sources.

Stop & Commit
- "Phase 3: evidence-grounded generation + deterministic citations"

───────────────────────────────────────────────────────────────────────────────

Phase 4 — Evaluation (small but powerful)
Goal
- Demonstrate retrieval quality (not just demos).

Tasks
1) eval/gold.json
- 10–30 queries + expected urls/doc_ids

2) eval/retrieval_eval.py
- Recall@k
- MRR

Files to touch
- eval/gold.json
- eval/retrieval_eval.py
- cli.py (optional: `eval-retrieval` command)

Definition of Done
- `python -m daily_art.eval.retrieval_eval` prints metrics (Recall@k, MRR).

Stop & Commit
- "Phase 5: retrieval evaluation harness"